{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from gensim.models import FastText\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts\n",
    "import sklearn.manifold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import multiprocessing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import time\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"weat.txt\", \"r\" ) \n",
    "file=file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "raw_sentences = tokenizer.tokenize(file.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file2 = open(\"wiki.txt\", \"r\",errors='ignore').read() #dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the corpus into sentences\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "raw_sentences2 = tokenizer.tokenize(file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus is now 539187177 characters long\n"
     ]
    }
   ],
   "source": [
    "print(\"Corpus is now {0} characters long\".format(len(file2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert into a list of words\n",
    "#remove unnecessary, split into words, no hyphens\n",
    "#list of words\n",
    "def sentence_to_wordlist(raw):\n",
    "    clean=re.sub('[\\n]',' ',raw.lower())\n",
    "    clean = re.sub(\"[^a-zA-Z]\",\" \", raw.lower())\n",
    "    words = clean.split()\n",
    "    #words = [ps.stem(w) for w in words if not w in stop_words]  \n",
    "    words = [w for w in words if not w in stop_words] \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence where each word is tokenized\n",
    "sentences = []\n",
    "for raw_sentence in raw_sentences2:\n",
    "    if len(raw_sentence) > 0:\n",
    "        sentences.append(sentence_to_wordlist(raw_sentence))\n",
    "#print(sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corpus contains 48,453,527 tokens\n"
     ]
    }
   ],
   "source": [
    "token_count = sum([len(sentence) for sentence in sentences])\n",
    "print(\"The corpus contains {0:,} tokens\".format(token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordList(sentences,minCount):\n",
    "    wordList = []\n",
    "    count = Counter()\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            count[word] += 1\n",
    "    for word,c in count.items():\n",
    "        if c >= minCount:\n",
    "            wordList.append(word)\n",
    "    return wordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordList=getWordList(sentences,minCount=5) # for min frequency of a word which should be 5\n",
    "#print(WordList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158189"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(WordList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences,size=300,window=5,min_count=5,seed=102,workers=multiprocessing.cpu_count(),batch_words=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Word2Vec(sentences,size=300,window=5,min_count=5,seed=42,workers=multiprocessing.cpu_count(),batch_words=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_frequency\n",
    "w2c = dict()\n",
    "for item in model.wv.vocab:\n",
    "    w2c[item]=model.wv.vocab[item].count\n",
    "w2cSorted=dict(sorted(w2c.items(), key=lambda x: x[1],reverse=True))\n",
    "# reverse output word and their frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stability2(word,similar1,similar2):    \n",
    "    overlap=0\n",
    "    for i in range(len(similar1)):\n",
    "        for j in range(len(similar2)):\n",
    "            if similar1[i][0] == similar2[j][0]:\n",
    "                overlap=overlap+1\n",
    "    return overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17569.609375 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.process_time()\n",
    "with open('stability_wiki_weat.csv', 'a',errors='replace') as the_file:\n",
    "    the_file.write('Word,Stability(%),Stability(%) IU,Frequency\\n')\n",
    "    for word in WordList:\n",
    "        similar1=model.wv.most_similar(word, topn=10)\n",
    "        similar2=model2.wv.most_similar(word, topn=10)\n",
    "        stab = Stability2(word,similar1,similar2)\n",
    "        the_file.write(word+','+str(stab*10)+','+str(stab*100/(20-stab))+','+str(w2cSorted[word])+'\\n')\n",
    "print (time.process_time() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('stability_wiki_weat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4959061.780428033\n"
     ]
    }
   ],
   "source": [
    "df['Stability(%) IU'] = pd.to_numeric(df['Stability(%) IU'])\n",
    "s = df['Stability(%) IU'].sum()\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_attribute(target_one,target_two, target_one_words, attribute_one,attribute_two, attribute_one_words, target_two_words, attribute_two_words):\n",
    "    cos=[]\n",
    "    s=0\n",
    "    s1=[]\n",
    "    s2=[]\n",
    "    S=[]\n",
    "    n=0\n",
    "    for i in range(0, len(target_one_words)):\n",
    "            c1=[]\n",
    "            c2=[]\n",
    "            for k in range(0, len(attribute_one_words)):\n",
    "                wt = target_one_words[i][:-1]\n",
    "                at1 = attribute_one_words[k][:-1]\n",
    "                try:\n",
    "                    cos1= model.similarity(wt, at1)\n",
    "                    cos.append(cos1)\n",
    "                    c1.append(cos1)\n",
    "                except:\n",
    "                    cos1=0\n",
    "                    cos.append(cos1)\n",
    "                    c1.append(cos1)\n",
    "                    continue\n",
    "            for k in range(0, len(attribute_two_words)):\n",
    "                cos2=0\n",
    "                wt = target_one_words[i][:-1]\n",
    "                at2 = attribute_two_words[k][:-1]\n",
    "                try:\n",
    "                    cos2= model.similarity(wt, at2)\n",
    "                    cos.append(cos2)\n",
    "                    c2.append(cos2)\n",
    "                except:\n",
    "                    cos2=0\n",
    "                    cos.append(cos2)\n",
    "                    c2.append(cos2)\n",
    "                    continue\n",
    "            s1.append((np.mean(c1)-np.mean(c2)))\n",
    "            S.append((np.mean(c1)-np.mean(c2)))\n",
    "            n=n+1\n",
    "    for i in range(0, len(target_two_words)):\n",
    "            c1=[]\n",
    "            c2=[]\n",
    "            for k in range(0, len(attribute_one_words)):\n",
    "                wt = target_two_words[i][:-1]\n",
    "                at1 = attribute_one_words[k][:-1]\n",
    "                try:\n",
    "                    cos1= model.similarity(wt, at1)\n",
    "                    cos.append(cos1)\n",
    "                    c1.append(cos1)\n",
    "                except:\n",
    "                    cos1=0\n",
    "                    cos.append(cos1)\n",
    "                    c1.append(cos1)\n",
    "                    continue\n",
    "            for k in range(0, len(attribute_two_words)):\n",
    "                cos2=0\n",
    "                wt = target_two_words[i][:-1]\n",
    "                at2 = attribute_two_words[k][:-1]\n",
    "                try:\n",
    "                    cos2= model.similarity(wt, at2)\n",
    "                    cos.append(cos2)\n",
    "                    c2.append(cos2)\n",
    "                except:\n",
    "                    cos2=0\n",
    "                    cos.append(cos2)\n",
    "                    c2.append(cos2)\n",
    "                    continue\n",
    "            s2.append((np.mean(c1)-np.mean(c2)))\n",
    "            S.append((np.mean(c1)-np.mean(c2)))\n",
    "    s=np.sum(s1)-np.sum(s2)\n",
    "    stdev=np.std(S)\n",
    "    print(target_one + ' vs ' + target_two  + ' , ' +attribute_one + ' vs ' + attribute_two +', d = ' + str(s/(stdev*n)))\n",
    "    #print(str(s/(stdev*n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-42-7381828b7dd7>:15: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  cos1= model.similarity(wt, at1)\n",
      "<ipython-input-42-7381828b7dd7>:28: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  cos2= model.similarity(wt, at2)\n",
      "<ipython-input-42-7381828b7dd7>:46: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  cos1= model.similarity(wt, at1)\n",
      "<ipython-input-42-7381828b7dd7>:59: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  cos2= model.similarity(wt, at2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flowers vs insects , pleasant vs unpleasant, d = 1.1862986653867436\n",
      "instruments vs weapons , pleasant vs unpleasant, d = 1.6614241015890672\n",
      "european_american_names vs african_american_names , pleasant vs unpleasant, d = 0.9290408857617427\n",
      "european_american_names vs african_american_names , pleasant vs unpleasant, d = 0.33907675519627123\n",
      "european_american_names vs african_american_names , pleasant vs unpleasant, d = 0.12110345187436487\n",
      "male_names vs female_names , career vs family, d = 1.9050925235393268\n",
      "math vs arts , male_terms vs female_terms, d = 1.7043706938451877\n",
      "science vs arts , male_terms vs female_terms, d = 1.004574865909872\n",
      "mental_disease vs physical_disease , temporary vs permanent, d = 1.6480578816065627\n",
      "young_people_names vs old_people_names , pleasant vs unpleasant, d = -0.7704346079420493\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(raw_sentences)-30):\n",
    "    words=raw_sentences[i*4].split()\n",
    "    target_one = words[0][:-1]\n",
    "    target_one_words = words[1:]\n",
    "    words1=raw_sentences[i*4+1].split()\n",
    "    target_two = words1[0][:-1]\n",
    "    target_two_words = words1[1:]\n",
    "    words2=raw_sentences[i*4+2].split()\n",
    "    attribute_one = words2[0][:-1]\n",
    "    attribute_one_words = words2[1:]\n",
    "    words3 = raw_sentences[i*4+3].split()\n",
    "    attribute_two = words3[0][:-1]\n",
    "    attribute_two_words = words3[1:]\n",
    "    \n",
    "    target_attribute(target_one,target_two, target_one_words, attribute_one,attribute_two, attribute_one_words, target_two_words, attribute_two_words)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

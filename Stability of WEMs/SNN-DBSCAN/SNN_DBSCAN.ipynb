{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import pickle\n",
    "import nltk\n",
    "from gensim.models import FastText\n",
    "from gensim.test.utils import common_texts\n",
    "import sklearn.manifold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import multiprocessing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import time\n",
    "import collections\n",
    "from scipy import stats \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"wiki.txt\", \"r\",errors='ignore').read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus is now 968283996 characters long\n"
     ]
    }
   ],
   "source": [
    "print(\"Corpus is now {0} characters long\".format(len(file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the corpus into sentences\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "raw_sentences = tokenizer.tokenize(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert into a list of words\n",
    "#remove unnecessary, split into words, no hyphens\n",
    "#list of words\n",
    "def sentence_to_wordlist(raw):\n",
    "    clean=re.sub('[\\n]',' ',raw.lower())\n",
    "    clean = re.sub(\"[^a-zA-Z]\",\" \", raw.lower())\n",
    "    words = clean.split()\n",
    "    #words = [ps.stem(w) for w in words if not w in stop_words]  \n",
    "    words = [w for w in words if not w in stop_words] \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence where each word is tokenized\n",
    "sentences = []\n",
    "for raw_sentence in raw_sentences:\n",
    "    if len(raw_sentence) > 0:\n",
    "        sentences.append(sentence_to_wordlist(raw_sentence))\n",
    "#print(sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corpus contains 81,592,322 tokens\n"
     ]
    }
   ],
   "source": [
    "token_count = sum([len(sentence) for sentence in sentences])\n",
    "print(\"The corpus contains {0:,} tokens\".format(token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordList(sentences,minCount):\n",
    "    wordList = []\n",
    "    count = Counter()\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            count[word] += 1\n",
    "    for word,c in count.items():\n",
    "        if c >= minCount:\n",
    "            wordList.append(word)\n",
    "    return wordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordList=getWordList(sentences,minCount=5) # for min frequency of a word which should be 5\n",
    "#print(WordList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51835"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vocab Size\n",
    "len(WordList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62960183"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "ab=list(itertools.chain.from_iterable(sentences))\n",
    "new_str =' '.join(ab)  \n",
    "open('wiki2.txt', 'w').write(new_str) # writing clean text to new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "783.703125 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.process_time()\n",
    "model1 = Word2Vec(sentences,size=300,window=5,min_count=5,seed=72,workers=multiprocessing.cpu_count(),batch_words=1) \n",
    "print (time.process_time() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "798.703125 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.process_time()\n",
    "model2 = Word2Vec(sentences,size=300,window=5,min_count=5,seed=82,workers=multiprocessing.cpu_count(),batch_words=1) \n",
    "print (time.process_time() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_frequency\n",
    "w2c = dict()\n",
    "for item in model1.wv.vocab:\n",
    "    w2c[item]=model1.wv.vocab[item].count\n",
    "w2cSorted=dict(sorted(w2c.items(), key=lambda x: x[1],reverse=True))\n",
    "# reverse output word and their frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stability(word,similar1,similar2):    \n",
    "    overlap=0\n",
    "    for i in range(len(similar1)):\n",
    "        for j in range(len(similar2)):\n",
    "            if similar1[i][0] == similar2[j][0]:\n",
    "                overlap=overlap+1\n",
    "    return overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "373.578125 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.process_time()\n",
    "with open('swiki.csv', 'a',errors='replace') as the_file:\n",
    "    the_file.write('Word,Stability(%),Stability(%) IU,Frequency\\n')\n",
    "    for word in WordList:\n",
    "        similar1=model1.wv.most_similar(word, topn=10)\n",
    "        similar2=model2.wv.most_similar(word, topn=10)\n",
    "        stab=Stability(word,similar1,similar2)\n",
    "        the_file.write(word+','+str(stab*10)+','+str(stab*100/(20-stab))+','+str(w2cSorted[word])+'\\n')\n",
    "print (time.process_time() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('swiki.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-126-5329cf6fff8b>:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  X = model1[model1.wv.vocab]\n"
     ]
    }
   ],
   "source": [
    "X = model1[model1.wv.vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-127-1b77f8722d0e>:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  Y = model2[model2.wv.vocab]\n"
     ]
    }
   ],
   "source": [
    "Y = model2[model2.wv.vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anborah\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SNN(min_shared_neighbor_proportion=None, neighbor_num=55)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClusterMixin\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "\n",
    "def snn(X, neighbor_num, min_shared_neighbor_num):\n",
    "    \"\"\"Perform Shared Nearest Neighbor (SNN) clustering algorithm clustering.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array or sparse (CSR) matrix of shape (n_samples, n_features), or array of shape (n_samples, n_samples)\n",
    "    A feature array\n",
    "    neighbor_num : int\n",
    "    K number of neighbors to consider for shared nearest neighbor similarity\n",
    "    min_shared_neighbor_num : int\n",
    "    Number of nearest neighbors that need to share two data points to be considered part of the same cluster\n",
    "    \"\"\"\n",
    "\n",
    "    # for each data point, find their set of K nearest neighbors\n",
    "    knn_graph = kneighbors_graph(X, n_neighbors=neighbor_num, include_self=False)\n",
    "    neighbors = np.array([set(knn_graph[i].nonzero()[1]) for i in range(len(X))])\n",
    "\n",
    "    # the distance matrix is computed as the complementary of the proportion of shared neighbors between each pair of data points\n",
    "    snn_distance_matrix = np.asarray([[get_snn_distance(neighbors[i], neighbors[j]) for j in range(len(neighbors))] for i in range(len(neighbors))])\n",
    "\n",
    "    # perform DBSCAN with the shared-neighbor distance criteria for density estimation\n",
    "    dbscan = DBSCAN(min_samples=min_shared_neighbor_num, metric=\"precomputed\")\n",
    "    dbscan = dbscan.fit(snn_distance_matrix)\n",
    "    return dbscan.core_sample_indices_, dbscan.labels_\n",
    "\n",
    "\n",
    "def get_snn_similarity(x0, x1):\n",
    "    \"\"\"Calculate the shared-neighbor similarity of two sets of nearest neighbors, normalized by the maximum number of shared neighbors\"\"\"\n",
    "\n",
    "    return len(x0.intersection(x1)) / len(x0)\n",
    "\n",
    "\n",
    "def get_snn_distance(x0, x1):\n",
    "    \"\"\"Calculate the shared-neighbor distance of two sets of nearest neighbors, normalized by the maximum number of shared neighbors\"\"\"\n",
    "\n",
    "    return 1 - get_snn_similarity(x0, x1)\n",
    "\n",
    "\n",
    "class SNN(BaseEstimator, ClusterMixin):\n",
    "    \"\"\"Class for performing the Shared Nearest Neighbor (SNN) clustering algorithm.\n",
    "    Parameters\n",
    "    ----------\n",
    "    neighbor_num : int\n",
    "        K number of neighbors to consider for shared nearest neighbor similarity\n",
    "    min_shared_neighbor_proportion : float [0, 1]\n",
    "        Proportion of the K nearest neighbors that need to share two data points to be considered part of the same cluster\n",
    "    Note: Naming conventions for attributes are based on the analogous ones of DBSCAN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, neighbor_num, min_shared_neighbor_proportion):\n",
    "\n",
    "        \"\"\"Constructor\"\"\"\n",
    "\n",
    "        self.neighbor_num = neighbor_num\n",
    "        self.min_shared_neighbor_num = round(neighbor_num * min_shared_neighbor_proportion)\n",
    "\n",
    "    def fit(self, X):\n",
    "\n",
    "        \"\"\"Perform SNN clustering from features or distance matrix.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array or sparse (CSR) matrix of shape (n_samples, n_features), or array of shape (n_samples, n_samples)\n",
    "            A feature array\n",
    "        \"\"\"\n",
    "\n",
    "        clusters = snn(X, neighbor_num=self.neighbor_num, min_shared_neighbor_num=self.min_shared_neighbor_num)\n",
    "        self.core_sample_indices_, self.labels_ = clusters\n",
    "        if len(self.core_sample_indices_):\n",
    "            # fix for scipy sparse indexing issue\n",
    "            self.components_ = X[self.core_sample_indices_].copy()\n",
    "        else:\n",
    "            # no core samples\n",
    "            self.components_ = np.empty((0, X.shape[1]))\n",
    "        return self\n",
    "\n",
    "    def fit_predict(self, X, y=None, sample_weight=None):\n",
    "        \"\"\"Performs clustering on X and returns cluster labels.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array or sparse (CSR) matrix of shape (n_samples, n_features), or \\\n",
    "                array of shape (n_samples, n_samples)\n",
    "            A feature array, or array of distances between samples if\n",
    "            ``metric='precomputed'``.\n",
    "        sample_weight : array, shape (n_samples,), optional\n",
    "            Weight of each sample, such that a sample with a weight of at least\n",
    "            ``min_samples`` is by itself a core sample; a sample with negative\n",
    "            weight may inhibit its eps-neighbor from being core.\n",
    "            Note that weights are absolute, and default to 1.\n",
    "        y : Ignored\n",
    "        Returns\n",
    "        -------\n",
    "        y : ndarray, shape (n_samples,)\n",
    "            cluster labels\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.labels_\n",
    "    \n",
    "\n",
    "snn1 = SNN(neighbor_num=55, min_shared_neighbor_proportion=0.5)\n",
    "snn1.fit(X)\n",
    "\n",
    "snn2 = SNN(neighbor_num=55, min_shared_neighbor_proportion=0.5)\n",
    "snn2.fit(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_samples_mask = np.zeros_like(snn1.labels_, dtype=bool)\n",
    "core_samples_mask[snn1.core_sample_indices_] = True\n",
    "labelsw1 = snn1.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_samples_mask2 = np.zeros_like(snn2.labels_, dtype=bool)\n",
    "core_samples_mask2[snn2.core_sample_indices_] = True\n",
    "labelsw2 = snn2.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjusted rand score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8426202511848865"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.adjusted_rand_score(labelsw1, labelsw3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
